{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP\n",
    " \n",
    "Sungchul Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Install RISE for an interactive view](https://github.com/damianavila/RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/move1.gif\" width=\"70%\" height=\"30%\"></div>\n",
    "\n",
    "https://qzprod.files.wordpress.com/2016/03/move1.gif?w=641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "- Data $\\{(x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a function $f$ that explains well the relation of all inputs $x^{(i)}$ and outputs $y^{(i)}$.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Data $\\{x^{(i)}\\}$\n",
    "- Goal: Find a structure that explains well data.\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Data $\\{x^{(i)}\\ \\mbox{or sometimes}\\ (x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a good policy that wins the game most of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.12.12 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/breakout.gif\" width=\"30%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Markov Decision Process 2.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.07.31 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.33.19 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.16.44 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.20.25 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.57.43 PM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.57.53 PM.png\" width=\"60%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.01 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.10 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.41 PM.png\" width=\"40%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP in Andrew Ng's Lecture 16 (CS229)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.09.22 PM.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "Ng\n",
    "[16](https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16)\n",
    "[demo](http://heli.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "0&1&2&3\\\\\n",
    "4&\\mbox{W}&5&6\\\\\n",
    "7&8&9&10\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0: Left\n",
    "\n",
    "- 1: Right\n",
    "\n",
    "- 2: Up\n",
    "\n",
    "- 3: Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You move according to your action with 80$\\%$ probability. \n",
    "\n",
    "- Your move may have a left and right one click error with 10$\\%$ probability each. \n",
    "\n",
    "- If there is a barrier against your move, your move bounds back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- -0.02 for each action.\n",
    "\n",
    "- If you reach the state 3, you win and get the final reward 1 at the end step.\n",
    "\n",
    "- If you reach the state 6, you lose and get the final reward -1 at the end step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1\n",
    "R[6,:] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\gamma=0.99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/2007-164-water-policy.jpg\" width=\"40%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2016-12-16+15.11.27.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Downarrow&\\mbox{W}&\\Rightarrow&-1\\\\\n",
    "\\Rightarrow&\\Rightarrow&\\Uparrow&\\Uparrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Uparrow&\\mbox{W}&\\Uparrow&-1\\\\\n",
    "\\Uparrow&\\Leftarrow&\\Leftarrow&\\Leftarrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate random samples from discrete distribution - np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x        = [ -3,  -1,   1,   2,   5]\n",
    "pmf      = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "x_sample = np.random.choice(x, p=pmf, size=(1000,)) \n",
    "\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-29 at 10.03.50 AM.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  8, a: 2, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 1, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 1, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 0, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 2, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 2, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 3, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 2, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 1, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 1, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 3, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 2, r: -0.02, s1:  5, done: 0, info: prob 0.25\n",
      "s:  5, a: 1, r: -0.02, s1:  6, done: 1, info: prob 0.25\n",
      "final reward -1 obtained and lose the game!\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 1: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# MDP simulation\n",
    "msg = \"s: {:2}, a: {}, r: {:5.2f}, s1: {:2}, done: {:1}, info: {}\" \n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "\n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # probaility of choosing action a under the current policy\n",
    "        prob = policy[s, a]\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # print current situation\n",
    "        msg_print = msg.format(s, a, R[s, a], s1, done, \"prob \"+str(prob))\n",
    "        print(msg_print)\n",
    "\n",
    "        if (s1 == 3):\n",
    "            # if win, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # print current game situation\n",
    "            # print final win comment\n",
    "            done = True\n",
    "            print('final reward 1 obtained and win the game!')\n",
    "        elif (s1 == 6):\n",
    "            # if lose, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # print current game situation\n",
    "            # print final lose comment\n",
    "            done = True\n",
    "            print('final reward -1 obtained and lose the game!')\n",
    "        else:\n",
    "            # if game is not over, \n",
    "            # print current game situation\n",
    "            # continue playing game\n",
    "            s = s1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy.png\" width=\"60%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# MDP simulation\n",
    "simulation_history = []\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "        if s1 == 3:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(1)\n",
    "        elif s1 == 6:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(0)\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "history = np.cumsum(simulation_history) / (np.arange(epoch) + 1)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "print(\"Success rate: {}\".format(history[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
