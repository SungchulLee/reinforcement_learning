{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP\n",
    " \n",
    "Sungchul Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Install RISE for an interactive view](https://github.com/damianavila/RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/move1.gif\" width=\"70%\" height=\"30%\"></div>\n",
    "\n",
    "https://qzprod.files.wordpress.com/2016/03/move1.gif?w=641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "- Data $\\{(x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a function $f$ that explains well the relation of all inputs $x^{(i)}$ and outputs $y^{(i)}$.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Data $\\{x^{(i)}\\}$\n",
    "- Goal: Find a structure that explains well data.\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Data $\\{x^{(i)}\\ \\mbox{or sometimes}\\ (x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a good policy that wins the game most of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.12.12 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/breakout.gif\" width=\"30%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Markov Decision Process 2.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.07.31 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.33.19 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.16.44 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 10.20.25 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.57.43 PM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.57.53 PM.png\" width=\"60%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.01 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.10 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-12-18 at 12.58.41 PM.png\" width=\"40%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP in Andrew Ng's Lecture 16 (CS229)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.09.22 PM.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "Ng\n",
    "[16](https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16)\n",
    "[demo](http://heli.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "0&1&2&3\\\\\n",
    "4&\\mbox{W}&5&6\\\\\n",
    "7&8&9&10\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0: Left\n",
    "\n",
    "- 1: Right\n",
    "\n",
    "- 2: Up\n",
    "\n",
    "- 3: Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You move according to your action with 80$\\%$ probability. \n",
    "\n",
    "- Your move may have a left and right one click error with 10$\\%$ probability each. \n",
    "\n",
    "- If there is a barrier against your move, your move bounds back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- -0.02 for each action.\n",
    "\n",
    "- If you reach the state 3, you win and get the final reward 1 at the end step.\n",
    "\n",
    "- If you reach the state 6, you lose and get the final reward -1 at the end step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\gamma=0.99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/2007-164-water-policy.jpg\" width=\"40%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2016-12-16+15.11.27.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Downarrow&\\mbox{W}&\\Rightarrow&-1\\\\\n",
    "\\Rightarrow&\\Rightarrow&\\Uparrow&\\Uparrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Uparrow&\\mbox{W}&\\Uparrow&-1\\\\\n",
    "\\Uparrow&\\Leftarrow&\\Leftarrow&\\Leftarrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate random samples from discrete distribution - np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x        = [ -3,  -1,   1,   2,   5]\n",
    "pmf      = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "x_sample = np.random.choice(x, p=pmf, size=(1000,)) \n",
    "\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-29 at 10.03.50 AM.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  7, a: 2, r: -0.02, s1:  8, done: 0, info: prob 1.0\n",
      "s:  8, a: 0, r: -0.02, s1:  7, done: 0, info: prob 1.0\n",
      "s:  7, a: 2, r: -0.02, s1:  4, done: 0, info: prob 1.0\n",
      "s:  4, a: 2, r: -0.02, s1:  0, done: 0, info: prob 1.0\n",
      "s:  0, a: 1, r: -0.02, s1:  4, done: 0, info: prob 1.0\n",
      "s:  4, a: 2, r: -0.02, s1:  0, done: 0, info: prob 1.0\n",
      "s:  0, a: 1, r: -0.02, s1:  1, done: 0, info: prob 1.0\n",
      "s:  1, a: 1, r: -0.02, s1:  2, done: 0, info: prob 1.0\n",
      "s:  2, a: 1, r: -0.02, s1:  3, done: 0, info: prob 1.0\n",
      "final reward 1 obtained and win the game!\n",
      "s:  9, a: 0, r: -0.02, s1:  5, done: 0, info: prob 1.0\n",
      "s:  5, a: 2, r: -0.02, s1:  2, done: 0, info: prob 1.0\n",
      "s:  2, a: 1, r: -0.02, s1:  3, done: 0, info: prob 1.0\n",
      "final reward 1 obtained and win the game!\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# MDP simulation\n",
    "msg = \"s: {:2}, a: {}, r: {:5.2f}, s1: {:2}, done: {:1}, info: {}\" \n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "\n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # probaility of choosing action a under the current policy\n",
    "        prob = policy[s, a]\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # print current situation\n",
    "        msg_print = msg.format(s, a, R[s, a], s1, done, \"prob \"+str(prob))\n",
    "        print(msg_print)\n",
    "\n",
    "        if (s1 == 3):\n",
    "            # if win, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # print current game situation\n",
    "            # print final win comment\n",
    "            done = True\n",
    "            print('final reward 1 obtained and win the game!')\n",
    "        elif (s1 == 6):\n",
    "            # if lose, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # print current game situation\n",
    "            # print final lose comment\n",
    "            done = True\n",
    "            print('final reward -1 obtained and lose the game!')\n",
    "        else:\n",
    "            # if game is not over, \n",
    "            # print current game situation\n",
    "            # continue playing game\n",
    "            s = s1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy.png\" width=\"60%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.944\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# MDP simulation\n",
    "simulation_history = []\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "        if s1 == 3:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(1.)\n",
    "        elif s1 == 6:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(0.)\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "history = np.cumsum(simulation_history) / (np.arange(epoch) + 1)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "print(\"Success rate: {}\".format(history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
