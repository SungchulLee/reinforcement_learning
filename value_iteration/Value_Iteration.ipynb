{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration\n",
    " \n",
    "Sungchul Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Value.jpg\" width=\"40%\" height=\"10%\"></div>\n",
    "\n",
    "http://seanheritage.com/wp-content/uploads/2017/03/Value.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value iteration for $v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $v_*(s)=0$ for all $s$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$  (synchronous or asynchronous) update $v_*$ using Bellman's optimality equation: \n",
    "\\begin{eqnarray*}\n",
    "v_*(s)&=&\\max_{a}\\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Find optimal policy $\\pi_*$ by solving\n",
    "$$\n",
    "\\pi_*(s)=\\mbox{argmax}_{a}q_*(s,a)\n",
    "$$\n",
    "    where\n",
    "$$\n",
    "q_*(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value iteration for $q_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Initialize $q_*(s,a)=0$ for all $s$ and $a$.\n",
    "\n",
    "- Repeat.\n",
    "\n",
    "    For every $s$ and $a$ (synchronous or asynchronous) update $q_*$ using Bellman's optimality equation: \n",
    "\\begin{eqnarray*}\n",
    "q_*(s,a)&=&{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}^a_{ss'}\\left(\\max_{a'}q_*(s',a')\\right)\\nonumber\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Find optimal policy $\\pi_*$ by solving\n",
    "$$\n",
    "\\pi_*(s)=\\mbox{argmax}_{a}q_*(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration for $v_*$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/cs188_mdp_optimal_policies.png\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Screenshot+2017-7.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.11015816  -6.94125631  -8.60073102 -10.50724629]\n",
      " [ -8.32961829  -4.20274388  -6.1870826   -6.1870826 ]\n",
      " [ -5.85112945  -1.7305563   -3.68767223  -5.12692146]\n",
      " [  1.           1.           1.           1.        ]\n",
      " [-11.13699055 -11.13699055  -9.34847257 -12.21752483]\n",
      " [ -5.56563998  -3.54779017  -3.82083182  -7.125952  ]\n",
      " [ -1.          -1.          -1.          -1.        ]\n",
      " [-12.33784195 -10.56379705 -11.27386647 -12.23640346]\n",
      " [-12.01464785  -8.32384136 -10.22276336 -10.22276336]\n",
      " [ -9.52817868  -5.90368784  -6.00490905  -7.87078   ]\n",
      " [ -7.14571971  -5.43799046  -3.74746404  -5.92345555]]\n",
      "\n",
      "[1 1 1 0 2 1 0 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Value iteration for $v_*$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -2. * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# value function\n",
    "# V = np.zeros(N_STATES)\n",
    "# V[3] = 1.\n",
    "# V[6] = -1.\n",
    "\n",
    "# # value iteration for V\n",
    "# for _ in range(100):\n",
    "#     for s in range(N_STATES):\n",
    "#         if (s!=3) and (s!=6):\n",
    "#             V[s] = max([R[s,a] + gamma * \\\n",
    "#                        sum([P[s,a,s1] * V[s1] \\\n",
    "#                            for s1 in range(N_STATES)]) \\\n",
    "#                        for a in range(N_ACTIONS)])\n",
    "\n",
    "# print(V)\n",
    "# print()\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1.\n",
    "Q[6,:] = -1.\n",
    "\n",
    "# compute Q\n",
    "for _ in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        if (s!=3) and (s!=6):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s,a] = R[s,a] + gamma * \\\n",
    "                         sum([P[s,a,s1] * max([Q[s1,a1] for a1 in range(N_ACTIONS)]) \\\n",
    "                             for s1 in range(N_STATES)])\n",
    "\n",
    "print(Q)\n",
    "print()\n",
    "\n",
    "optimal_policy = np.argmax(Q, axis=1)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value iteration for $q_*$ in Andrew Ng's lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/cs188_mdp_optimal_policies.png\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "https://raw.githubusercontent.com/mebusy/notes/master/imgs/cs188_mdp_optimal_policies.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Screenshot+2017-3.png\" width=\"100%\" height=\"10%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82322354  0.85530117  0.83075787  0.80256088]\n",
      " [ 0.83476757  0.89580324  0.86645526  0.86645526]\n",
      " [ 0.84984258  0.93236641  0.90611872  0.71218162]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.79112222  0.79112222  0.81969892  0.76026732]\n",
      " [ 0.68696646 -0.64953064  0.68749634  0.5103828 ]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.75636299  0.72890705  0.78026128  0.74902668]\n",
      " [ 0.74559468  0.68894841  0.71792194  0.71792194]\n",
      " [ 0.70873821  0.50703739  0.64691224  0.66373581]\n",
      " [ 0.49092193  0.31841144 -0.69323365  0.48757652]]\n",
      "\n",
      "[1 1 1 0 2 2 0 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Value iteration for $q_*$ in Andrew Ng's lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# state\n",
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0,1,2,3] # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# value function\n",
    "# V = np.zeros(N_STATES)\n",
    "# V[3] = 1.\n",
    "# V[6] = -1.\n",
    "\n",
    "# # value iteration for V\n",
    "# for _ in range(100):\n",
    "#     for s in range(N_STATES):\n",
    "#         if (s!=3) and (s!=6):\n",
    "#             V[s] = max([R[s,a] + gamma * \\\n",
    "#                        sum([P[s,a,s1] * V[s1] \\\n",
    "#                            for s1 in range(N_STATES)]) \\\n",
    "#                        for a in range(N_ACTIONS)])\n",
    "\n",
    "# print(V)\n",
    "# print()\n",
    "\n",
    "# Q function\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1.\n",
    "Q[6,:] = -1.\n",
    "\n",
    "# compute Q\n",
    "for _ in range(100):\n",
    "    for s in range(N_STATES):\n",
    "        if (s!=3) and (s!=6):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q[s,a] = R[s,a] + gamma * \\\n",
    "                         sum([P[s,a,s1] * max([Q[s1,a1] for a1 in range(N_ACTIONS)]) \\\n",
    "                             for s1 in range(N_STATES)])\n",
    "\n",
    "print(Q)\n",
    "print()\n",
    "\n",
    "optimal_policy = np.argmax(Q, axis=1)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "[0. 0. 1. 0.]\n",
      "\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n",
      "\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.eye(4))\n",
    "print()\n",
    "print(np.eye(4)[2])\n",
    "print()\n",
    "print(np.eye(4)[[0,0,3]])\n",
    "print()\n",
    "print(np.eye(4)[[0,0,3]].astype(np.int32))\n",
    "print()\n",
    "print(np.eye(4)[[0,0,3]].astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
