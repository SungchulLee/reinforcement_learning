{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman's Equation\n",
    " \n",
    "Sungchul Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sutton \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/bookdraft2017nov5.pdf)\n",
    "Szepesv√°ri \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)\n",
    "Ng\n",
    "[16](https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16)\n",
    "[demo](http://heli.stanford.edu/) \n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- [Install RISE for an interactive presentation viewer](https://github.com/damianavila/RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/2007-164-water-policy.jpg\" width=\"40%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screenshot+2016-12-16+15.11.27.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Downarrow&\\mbox{W}&\\Rightarrow&-1\\\\\n",
    "\\Rightarrow&\\Rightarrow&\\Uparrow&\\Uparrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "(11, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_STATES = 11\n",
    "N_ACTIONS = 4\n",
    "\n",
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]\n",
    "policy = policy.astype(np.float32)\n",
    "\n",
    "print(policy)\n",
    "print(type(policy))\n",
    "print(policy.dtype)\n",
    "print(policy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "(11, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_STATES = 11\n",
    "N_ACTIONS = 4\n",
    "\n",
    "policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "policy = policy.astype(np.float32)\n",
    "\n",
    "print(policy)\n",
    "print(type(policy))\n",
    "print(policy.dtype)\n",
    "print(policy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Uparrow&\\mbox{W}&\\Uparrow&-1\\\\\n",
    "\\Uparrow&\\Leftarrow&\\Leftarrow&\\Leftarrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "(11, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_STATES = 11\n",
    "N_ACTIONS = 4\n",
    "\n",
    "policy = np.empty((N_STATES, N_ACTIONS))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]\n",
    "policy = policy.astype(np.float32)\n",
    "\n",
    "print(policy)\n",
    "print(type(policy))\n",
    "print(policy.dtype)\n",
    "print(policy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value function and Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/Q-function.png\" width=\"70%\" height=\"10%\"></div>\n",
    "\n",
    "https://www.youtube.com/watch?v=Vd-gmo-qO5E&index=4&list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{lllll}\n",
    "\\mbox{Value function}&&\\quad v_\\pi(s)&=&E_\\pi(G_t|S_t=s)\\\\\n",
    "\\mbox{Action-value function or Q function}&&\\quad q_\\pi(s,a)&=&E_\\pi(G_t|S_t=s,A_t=a)\n",
    "\\end{array}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots++\\gamma^{T-t} R_{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"img/nature14236-sf2.jpg\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "https://images.nature.com/full/nature-assets/nature/journal/v518/n7540/images/nature14236-sf2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman's expectation equation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.35 PM.png\" width=\"50%\" height=\"30%\"></div>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.43.46 PM.png\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal policy, value function, and action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/cs188_mdp_policy_methods.png\" width=\"50%\"></div>\n",
    "\n",
    "https://github.com/mebusy/notes/blob/master/dev_notes/AI_CS188_MDP.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Optimal Policy 1.png\"/>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/Optimal Policy 3.png\"/>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman's optimality equation \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.44.18 PM.png\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2017-11-18 at 8.44.24 PM.png\" width=\"50%\" height=\"10%\"></div>\n",
    "\n",
    "Silver \n",
    "[2](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_2.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
