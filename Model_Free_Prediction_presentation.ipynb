{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Free Prediction\n",
    " \n",
    "Sungchul Lee "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- [Install RISE for an interactive presentation viewer](https://github.com/damianavila/RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model vs Model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{llllll}\n",
    "\\mbox{Model}&\\quad\\Rightarrow\\quad&\\mbox{Model-free}\\\\\n",
    "\\mbox{Based on $P_{ss'}^a$}&\\quad\\Rightarrow\\quad&\\mbox{Based on Samples}\\\\\n",
    "V&\\quad\\Rightarrow\\quad&Q\\\\\n",
    "\\mbox{Greedy}&\\quad\\Rightarrow\\quad&\\mbox{$\\varepsilon$-Greedy}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use $Q$ instead of $V$ if don't know the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Model\n",
    "\n",
    "If we know $R_s^a$, $P_{ss'}^a$, and $V$, and if we are at state $s$, our next action is\n",
    "$$\n",
    "\\mbox{argmax}_a Q(s,a) \\quad =\\quad \\mbox{argmax}_a\\left(R_s^a + \\gamma * \\sum_{s'} P_{ss'}^a * V(s')\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Model-free\n",
    "\n",
    "In reality, typically we don't know $P_{ss'}^a$.\n",
    "So, we cannot decide our next action based on $V$.\n",
    "That is why we use $Q$, not $V$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use $\\varepsilon$-greedy policy update instead of greedy policy update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Model\n",
    "If we know $R_s^a$, $P_{ss'}^a$, and $V$, every states $s$ are counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Model-free\n",
    "- In reality, typically we don't know $P_{ss'}^a$.\n",
    "- So, we take samples.\n",
    "- If we update policy greedily, we may miss good regions in state space.\n",
    "- That is why we use $\\varepsilon$-greedy policy update instead of greedy policy update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\varepsilon$-greedy policy update\n",
    "- Update $\\varepsilon$-greedy policy using udated $Q$\n",
    "$$\n",
    "\\pi(a|s)=\\left\\{\\begin{array}{ll}\n",
    "\\frac{\\varepsilon}{m}+(1-\\varepsilon)&\\mbox{if}\\ a=\\mbox{argmax}_{a'}Q(s,a')\\\\\n",
    "\\frac{\\varepsilon}{m}&\\mbox{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "where $m$ is the number of actions.\n",
    "- Choose action using updated $\\varepsilon$-greedy policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On and Off-Policy Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### On-policy learning\n",
    "\n",
    "- “Learn on the job”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\pi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Off-policy learning\n",
    "\n",
    "- “Look over someone’s shoulder”\n",
    "- Learn about policy $\\pi$ from experience sampled from $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{array}{ccc}\\hline\n",
    "\\mbox{Prediction (on-policy)}&\\mbox{Control (on-policy)}&\\mbox{Control (off-policy)}\\\\\\hline\n",
    "\\mbox{MC}&\\mbox{MC}\\\\\n",
    "\\mbox{TD}&\\mbox{SARSA}&\\mbox{Q-learnig}\\\\\n",
    "\\mbox{TD($\\lambda$)}&\\mbox{SARSA($\\lambda$)}\\\\\\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-28 at 12.39.39 PM.png\" width=\"50%\"></div>\n",
    "\n",
    "Silver\n",
    "[4](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-28 at 12.40.02 PM.png\" width=\"50%\"></div>\n",
    "\n",
    "Silver\n",
    "[4](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-28 at 12.40.14 PM.png\" width=\"50%\"></div>\n",
    "\n",
    "Silver\n",
    "[4](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0398     -0.0398      0.71276807  1.         -0.0398     -0.15055206\n",
      " -1.         -0.0398     -0.0398     -0.0398     -0.14695207]\n"
     ]
    }
   ],
   "source": [
    "# Every-Visit Monte-Carlo Policy Evaluation - V\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# Every-Visit Monte-Carlo Policy Evaluation\n",
    "# n_visits records number of visits for each state\n",
    "# cum_gains records cumulative gains, i.e., sum of gains for each state\n",
    "# where\n",
    "# gain = reward + gamma * next_reward + gamma^2 * ...\n",
    "n_visits = np.zeros(N_STATES)\n",
    "cum_gains = np.zeros(N_STATES)\n",
    "for _ in range(epoch):\n",
    "    # simulation_history records visited states including the terminal states 3 and 6\n",
    "    # reward_history records occured rewards including final rewards 1. and -1. \n",
    "    simulation_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        simulation_history.append(s)\n",
    "        reward_history.append(R[s,a])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "        if s1 == 3:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(s1)\n",
    "            reward_history.append(R[s1,0])\n",
    "        elif s1 == 6:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(s1)\n",
    "            reward_history.append(R[s1,0])\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "    # reward_history records occured rewards including final rewards 1 and -1  \n",
    "    simulation_history = np.array(simulation_history)\n",
    "    reward_history = np.array(reward_history)\n",
    "    n = len(reward_history)\n",
    "    \n",
    "    # gain_history records occured gains \n",
    "    gain_history = deepcopy(reward_history) \n",
    "    for i, reward in enumerate(reward_history[:-1][::-1]): \n",
    "        gain_history[n-i-2] = reward + gamma * reward_history[n-i-2+1]\n",
    "        \n",
    "    # update n_visits and cum_gains     \n",
    "    for i in range(N_STATES):\n",
    "        n_visits[i] += np.sum(simulation_history==i) \n",
    "        cum_gains[i] += np.sum(gain_history[simulation_history==i]) \n",
    "                        \n",
    "V = cum_gains / (n_visits + 1.0e-8) \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.0398      0.76412426  0.09985319  0.04644626]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.0398     -0.81098462 -0.13773714 -0.13283288]\n",
      " [-1.          0.          0.          0.        ]\n",
      " [-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.0398     -0.0398     -0.0398     -0.0398    ]\n",
      " [-0.13948329 -0.15811707 -0.82644865 -0.0398    ]]\n"
     ]
    }
   ],
   "source": [
    "# Every-Visit Monte-Carlo Policy Evaluation - Q\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# Every-Visit Monte-Carlo Policy Evaluation\n",
    "# n_visits records number of visits for each state and action\n",
    "# cum_gains records cumulative gains, i.e., sum of gains for each state and action\n",
    "# where\n",
    "# gain = reward + gamma * next_reward + gamma^2 * ...\n",
    "\n",
    "# Previously for V\n",
    "# n_visits = np.zeros(N_STATES)\n",
    "# cum_gains = np.zeros(N_STATES)\n",
    "\n",
    "n_visits = np.zeros((N_STATES, N_ACTIONS))\n",
    "cum_gains = np.zeros((N_STATES, N_ACTIONS))\n",
    "\n",
    "for _ in range(epoch):\n",
    "    # simulation_history records visited states and actions including the terminal states 3 and 6\n",
    "    # reward_history records occured rewards including final rewards 1 and -1 \n",
    "    simulation_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # Previously for V \n",
    "        # simulation_history.append(s)\n",
    "        \n",
    "        simulation_history.append((s,a))\n",
    "        reward_history.append(R[s,a])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "        if s1 == 3:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            \n",
    "            # Previously for V \n",
    "            # simulation_history.append(s1)\n",
    "            \n",
    "            simulation_history.append((s1,0))\n",
    "            reward_history.append(R[s1,0])\n",
    "        elif s1 == 6:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            \n",
    "            # Previously for V \n",
    "            # simulation_history.append(s1)\n",
    "            \n",
    "            simulation_history.append((s1,0))\n",
    "            reward_history.append(R[s1,0])\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "    # reward_history records occured rewards including final rewards 1 and -1  \n",
    "    reward_history = np.array(reward_history)\n",
    "    n = len(reward_history)\n",
    "    \n",
    "    # gain_history records occured gains \n",
    "    gain_history = deepcopy(reward_history) \n",
    "    for i, reward in enumerate(reward_history[:-1][::-1]): \n",
    "        gain_history[n-i-2] = reward + gamma * reward_history[n-i-2+1]\n",
    "        \n",
    "    # update n_visits and cum_gains \n",
    "    \n",
    "    # Previously for V \n",
    "    # for i in range(N_STATES):\n",
    "    #     n_visits[i] += np.sum(simulation_history==i) \n",
    "    #     cum_gains[i] += np.sum(gain_history[simulation_history==i]) \n",
    "    \n",
    "    for i, (s, a) in enumerate(simulation_history): \n",
    "        n_visits[s, a] += 1.\n",
    "        cum_gains[s, a] += gain_history[i] \n",
    "                        \n",
    "Q = cum_gains / (n_visits + 1.0e-8) \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-28 at 2.49.15 PM.png\" width=\"50%\"></div>\n",
    "\n",
    "Silver\n",
    "[4](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-01-28 at 2.49.39 PM.png\" width=\"50%\"></div>\n",
    "\n",
    "Silver\n",
    "[4](https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal-Difference (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### MC\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha(\\color{red}{G_t}-V(S_t))\n",
    "$$\n",
    "- Use actual return $G_t$\n",
    "- Have to wait the full episode finishes to get $G_t$\n",
    "\n",
    "##### TD\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha(\\color{red}{R_{t+1}+\\gamma V(S_{t+1})}-V(S_t))\n",
    "$$\n",
    "$$\n",
    "Q(S_t,a_t)\\quad\\leftarrow\\quad Q(S_t,a_t)+\\alpha(\\color{red}{R_{t+1}+\\gamma Q(S_{t+1},a_{t+1})}-Q(S_t,a_t))\n",
    "$$\n",
    "- Use estimated return $R_{t+1}+\\gamma V(S_{t+1})$\n",
    "- No need to wait\n",
    "- $R_{t+1}+\\gamma V(S_{t+1})$ is called TD target \n",
    "- $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ is called TD error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8332311   0.88097326  0.92935936  1.          0.7958952   0.68854948\n",
      " -1.          0.75054477  0.70640766  0.66358746  0.45256953]\n"
     ]
    }
   ],
   "source": [
    "# TD - V\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# TD\n",
    "alpha = 0.01\n",
    "\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.\n",
    "V[6] = -1.\n",
    "\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # TD\n",
    "        # V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t))\n",
    "        V[s] += alpha * (R[s,a] + gamma * V[s1] - V[s]) \n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58148288  0.803039    0.63159061  0.59416368]\n",
      " [ 0.68858303  0.86196055  0.70202799  0.72653315]\n",
      " [ 0.7530652   0.91800854  0.81770306  0.59890602]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.54857856  0.56247567  0.75772864  0.50760254]\n",
      " [ 0.30556192 -0.41059434  0.65335226  0.12818476]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.4024784   0.35742952  0.68964561  0.43031629]\n",
      " [ 0.63128615  0.27340146  0.29428523  0.31911985]\n",
      " [ 0.56454013  0.09958052  0.18294806  0.17481447]\n",
      " [ 0.31013811 -0.02549665 -0.21913738  0.01936028]]\n"
     ]
    }
   ],
   "source": [
    "# TD - Q\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# TD\n",
    "alpha = 0.01\n",
    "\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3, :] = 1.\n",
    "Q[6, :] = -1.\n",
    "\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10]) \n",
    "    \n",
    "    # choose action using current policy\n",
    "    a = np.random.choice(actions, p=policy[s, :])\n",
    "    \n",
    "    while not done:\n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = np.random.choice(actions, p=policy[s1, :])\n",
    "        \n",
    "        # TD\n",
    "        \n",
    "        # Previously for V\n",
    "        # V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t))\n",
    "        # V[s] += alpha * (R[s,a] + gamma * V[s1] - V[s])\n",
    "        \n",
    "        Q[s,a] += alpha * (R[s,a] + gamma * Q[s1,a1] - Q[s,a])\n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias/Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bias\n",
    "- Return $G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-(t+1)}R_T$ is unbiased estimate of $v_\\pi(S_t)$\n",
    "- True TD target $R_{t+1} + \\gamma v_\\pi (S_{t+1})$ is unbiased estimate of $v_\\pi(S_t)$\n",
    "- TD target $R_{t+1} + \\gamma V(S_{t+1})$ is biased estimate of $v_\\pi(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Variance Trade-Off\n",
    "- TD target is much lower variance than the return:\n",
    "- Return depends on many random actions, transitions, rewards\n",
    "- TD target depends on one random action, transition, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2b31aa6aa8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0mV_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mV_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_MC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f2b31aa6aa8a>\u001b[0m in \u001b[0;36mV_MC\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# choose action using current policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0msimulation_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mreward_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "def V_MC():\n",
    "    \n",
    "    # n_visits records number of visits for each state\n",
    "    # cum_gains records cumulative gains, i.e., sum of gains for each state\n",
    "    # where\n",
    "    # gain = reward + gamma * next_reward + gamma^2 * ...\n",
    "\n",
    "    n_visits = np.zeros(N_STATES)\n",
    "    cum_gains = np.zeros(N_STATES)\n",
    "\n",
    "    for _ in range(epoch):\n",
    "\n",
    "        # simulation_history records visited states including the terminal states 3 and 6\n",
    "        # reward_history records occured rewards including final rewards 1. and -1. \n",
    "        simulation_history = []\n",
    "        reward_history = []\n",
    "\n",
    "        # indicate game is not over yet\n",
    "        done = False\n",
    "\n",
    "        # choose initial state randomly, not from 3 or 6\n",
    "        s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # choose action using current policy\n",
    "            a = np.random.choice(actions, p=policy[s, :])\n",
    "            simulation_history.append(s)\n",
    "            reward_history.append(R[s,a])\n",
    "\n",
    "            # choose next state using transition probabilities\n",
    "            s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "            if s1 == 3:\n",
    "                # if game is over, \n",
    "                # ready to break while loop by letting done = True\n",
    "                # append end result to simulation_history \n",
    "                done = True\n",
    "                simulation_history.append(s1)\n",
    "                reward_history.append(R[s1,0])\n",
    "            elif s1 == 6:\n",
    "                # if game is over, \n",
    "                # ready to break while loop by letting done = True\n",
    "                # append end result to simulation_history \n",
    "                done = True\n",
    "                simulation_history.append(s1)\n",
    "                reward_history.append(R[s1,0])\n",
    "            else:\n",
    "                # if game is not over, continue playing game\n",
    "                s = s1\n",
    "\n",
    "        # reward_history records occured rewards including final rewards 1 and -1  \n",
    "        simulation_history = np.array(simulation_history)\n",
    "        reward_history = np.array(reward_history)\n",
    "        n = len(reward_history)\n",
    "\n",
    "        # gain_history records occured gains \n",
    "        gain_history = deepcopy(reward_history) \n",
    "        for i, reward in enumerate(reward_history[:-1][::-1]): \n",
    "            gain_history[n-i-2] = reward + gamma * gain_history[n-i-2+1]\n",
    "\n",
    "        # update n_visits and cum_gains     \n",
    "        for i in range(N_STATES):\n",
    "            n_visits[i] += np.sum(simulation_history==i) \n",
    "            cum_gains[i] += np.sum(gain_history[simulation_history==i]) \n",
    "\n",
    "    V = cum_gains / (n_visits + 1.0e-8) \n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "V_history = np.empty((1000, N_STATES))\n",
    "for i in range(1000):\n",
    "    V_history[i, :] = V_MC()\n",
    "    \n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.3)\n",
    "fig.suptitle(\"Histogram of V_MC\", fontsize=16)\n",
    "\n",
    "for s in range(N_STATES):\n",
    "    ax = fig.add_subplot(3, 4, s + 1)\n",
    "    bins = np.linspace(-1.1,1.1,100)\n",
    "    label = \"state {}\".format(str(s))\n",
    "    ax.hist(V_history[:, s], bins=bins, label=label)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "alpha = 0.01\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# TD\n",
    "def V_TD():\n",
    "    \n",
    "    V = np.zeros(N_STATES)\n",
    "    V[3] = 1.\n",
    "    V[6] = -1.\n",
    "\n",
    "    for _ in range(epoch):\n",
    "\n",
    "        # indicate game is not over yet\n",
    "        done = False\n",
    "\n",
    "        # choose initial state randomly, not from 3 or 6\n",
    "        s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "\n",
    "        while not done:\n",
    "            # choose action using current policy\n",
    "            a = np.random.choice(actions, p=policy[s, :])\n",
    "\n",
    "            # choose next state using transition probabilities\n",
    "            s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "            # TD\n",
    "            # V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t))\n",
    "            V[s] += alpha * (R[s,a] + gamma * V[s1] - V[s]) \n",
    "\n",
    "            if (s1 == 3) or (s1 == 6):\n",
    "                # if game is over, \n",
    "                # ready to break while loop by letting done = True\n",
    "                done = True\n",
    "            else:\n",
    "                # if game is not over, continue playing game\n",
    "                s = s1\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "V_history = np.empty((1000, N_STATES))\n",
    "for i in range(1000):\n",
    "    V_history[i, :] = V_TD()\n",
    "    \n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.3)\n",
    "fig.suptitle(\"Histogram of V_TD\", fontsize=16)\n",
    "\n",
    "for s in range(N_STATES):\n",
    "    ax = fig.add_subplot(3, 4, s + 1)\n",
    "    bins = np.linspace(-1.1,1.1,100)\n",
    "    label = \"state {}\".format(str(s))\n",
    "    ax.hist(V_history[:, s], bins=bins, label=label)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TD\n",
    "$$\n",
    "V(S_t)\\quad\\leftarrow\\quad V(S_t)+\\alpha\\delta_t \n",
    "$$\n",
    "- for each step TD error $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ update only one $V(s)$, i.e., $V(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TD($\\lambda$)\n",
    "$$\\begin{array}{lll}\n",
    "E_0(s)&=&0\\\\\n",
    "E_t(s)&=&\\lambda\\gamma E_{t-1}(s)+1(S_t=s)\\\\\n",
    "V(s)&\\leftarrow&V(s)+\\alpha\\delta_t \\color{red}{E_t(s)}\n",
    "\\end{array}$$\n",
    "\n",
    "- $E_t(s)$ is called Eligibility Trace\n",
    "\n",
    "- for each step TD error $\\delta_t=R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$ update all $V(s)$ using eligibility trace $E_t(s)$\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "E_0(s,a)&=&0\\\\\n",
    "E_t(s,a)&=&\\lambda\\gamma E_{t-1}(s,a)+1(S_t=s,A_t=a)\\\\\n",
    "Q(s,a)&\\leftarrow&Q(s,a)+\\alpha\\delta_t \\color{red}{E_t(s,a)}\n",
    "\\end{array}$$\n",
    "\n",
    "- $E_t(s,a)$ is called Eligibility Trace\n",
    "\n",
    "- for each step TD error $\\delta_t=R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)$ update all $Q(s,t)$ using eligibility trace $E_t(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{lll}\n",
    "G_t^{(n)}\n",
    "&=&R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1}R_{t+n}+\\gamma^n V(S_{t+n})\\\\\n",
    "&=&R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1}(R_{t+n}+\\gamma V(S_{t+n}))\\\\\n",
    "&=&R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1} V(S_{t+n-1})+\\gamma^{n-1}(R_{t+n}+\\gamma V(S_{t+n}-V(S_{t+n-1}))\\\\\n",
    "&=&R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1} V(S_{t+n-1})+\\gamma^{n-1}\\delta_{t+n-1}\\\\\n",
    "&=&V(S_t)+\\delta_{t}+\\gamma \\delta_{t+1}+\\gamma^2 \\delta_{t+1}+\\cdots+\\gamma^{n-1}\\delta_{t+n-1}\\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\\begin{array}{lll}\n",
    "G_t^\\lambda\n",
    "&=&\\sum_{n=1}^\\infty (1-\\lambda)\\lambda^{n-1}G_t^{(n)}\\\\\n",
    "&=&V(S_t)+\\delta_{t}+\\lambda\\gamma \\delta_{t+1}+\\lambda^2\\gamma^2\\delta_{t+2}+\\cdots+\\lambda^{T-t-1}\\gamma^{T-t-1}\\delta_{T-1}\\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lll}\n",
    "\\sum_{t=1}^T\\alpha\\left(G_t^\\lambda-V(S_t)\\right)1(S_t=s)\n",
    "&=&\n",
    "\\sum_{t=1}^T\\alpha\\left(\\delta_{t}+\\lambda\\gamma \\delta_{t+1}+\\lambda^2\\gamma^2\\delta_{t+2}+\\cdots+\\lambda^{T-t-1}\\gamma^{T-t-1}\\delta_{T-1}\\right)1(S_t=s)\\\\\n",
    "&=&\n",
    "\\sum_{t=1}^T\\alpha\\delta_{t}1(S_t=s)\n",
    "+\\sum_{t=1}^T\\alpha\\lambda\\gamma \\delta_{t+1}1(S_t=s)\n",
    "+\\cdots\n",
    "+\\sum_{t=1}^T\\alpha\\lambda^{T-t-1}\\gamma^{T-t-1}\\delta_{T-1}1(S_t=s)\\\\\n",
    "&=&\n",
    "\\sum_{t=1}^T\\alpha\\delta_{t}E_t(s)\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.35923685 -0.15463591  0.08569467  1.         -0.51637213 -0.47188244\n",
      " -1.         -0.59896235 -0.64886068 -0.66913671 -0.84581493]\n"
     ]
    }
   ],
   "source": [
    "# TD(la) - V\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 1: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "# TD(la)\n",
    "alpha = 0.01\n",
    "la = 0.1\n",
    "\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.\n",
    "V[6] = -1.\n",
    "\n",
    "# eligibility trace\n",
    "# E_0(s)=0\n",
    "E =  np.zeros(N_STATES) \n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10]) \n",
    "    \n",
    "    # E_t(s) = \\gamma\\lambda E_{t-1}(s)+1(S_t=s)    \n",
    "    E = gamma * la * E\n",
    "    E[s] += 1\n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # TD(la)\n",
    "        # V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t)) E_t(s)\n",
    "        delta = R[s,a] + gamma * V[s1] - V[s]\n",
    "        for s2 in states: \n",
    "            if (s2!=3) and (s2!=6):\n",
    "                V[s2] = V[s2] + alpha * delta * E[s2] \n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "            \n",
    "            # E_t(s) = \\gamma\\lambda E_{t-1}(s)+1(S_t=s)    \n",
    "            E = gamma * la * E\n",
    "            E[s] += 1.\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46905959 -0.31846341 -0.4370555  -0.55149334]\n",
      " [-0.42965386 -0.10728225 -0.28342492 -0.26667498]\n",
      " [-0.28853779  0.72902271  0.09441897 -0.45228941]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [-0.61788721 -0.62228465 -0.49482378 -0.70402419]\n",
      " [-0.58385044 -0.92829749 -0.1833864  -0.77951916]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [-0.71915059 -0.76284053 -0.65111614 -0.74477241]\n",
      " [-0.74516016 -0.77649893 -0.78011129 -0.77962827]\n",
      " [-0.77015359 -0.8502401  -0.65441893 -0.79531558]\n",
      " [-0.81282693 -0.9306029  -0.97724453 -0.89762581]]\n"
     ]
    }
   ],
   "source": [
    "# TD(la) - Q\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "la = 0.1\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "N_STATES = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((N_STATES, N_ACTIONS, N_STATES))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((N_STATES, N_ACTIONS)) \n",
    "R[3,:] = 1.\n",
    "R[6,:] = -1.\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 1: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((N_STATES, N_ACTIONS))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((N_STATES, N_ACTIONS))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((N_STATES, N_ACTIONS))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((N_STATES,1))\n",
    "\n",
    "\" TD(la) \"\n",
    "\n",
    "# Q\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "Q[3,:] = 1.\n",
    "Q[6,:] = -1.\n",
    "\n",
    "# eligibility trace\n",
    "# E_0(s,a)=0\n",
    "E =  np.zeros((N_STATES, N_ACTIONS)) \n",
    "\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10]) \n",
    "    \n",
    "    # choose action using current policy\n",
    "    a = np.random.choice(actions, p=policy[s, :])\n",
    "    \n",
    "    # E_t(s2,a2) = \\gamma\\lambda E_{t-1}(s2,a2)+1(s2=s,a2=a)    \n",
    "    E = gamma * la * E\n",
    "    E[s,a] += 1.\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # choose action using current policy\n",
    "        a1 = np.random.choice(actions, p=policy[s1, :])\n",
    "        \n",
    "        # TD\n",
    "        # V(S_t)\\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1})-V(S_t))\n",
    "        # V[s] = V[s] + alpha * (R[s,a] + gamma * V[s1] - V[s])\n",
    "        # TD(lambda)\n",
    "        delta = R[s,a] + gamma * Q[s1,a1] - Q[s,a]\n",
    "        for s2 in states:\n",
    "            if (s2!=3) and (s2!=6):\n",
    "                for a2 in actions:\n",
    "                    Q[s2,a2] = Q[s2,a2] + alpha * delta * E[s2,a2]\n",
    "\n",
    "        if (s1 == 3) or (s1 == 6):\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "            a = a1\n",
    "            \n",
    "            # E_t(s,a) = \\gamma\\lambda E_{t-1}(s)+1(S_t=s)    \n",
    "            E = gamma * la * E\n",
    "            E[s,a] += 1.\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
