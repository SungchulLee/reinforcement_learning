{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP\n",
    " \n",
    "Sungchul Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Install RISE for an interactive view](https://github.com/damianavila/RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/move1.gif\" width=\"70%\" height=\"30%\"></div>\n",
    "\n",
    "https://qzprod.files.wordpress.com/2016/03/move1.gif?w=641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "- Data $\\{(x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a function $f$ that explains well the relation of all inputs $x^{(i)}$ and outputs $y^{(i)}$.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Data $\\{x^{(i)}\\}$\n",
    "- Goal: Find a structure that explains well data.\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Data $\\{x^{(i)}\\ \\mbox{or sometimes}\\ (x^{(i)},y^{(i)})\\}$\n",
    "- Goal: Find a good policy that wins the game most of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 10.12.12 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/breakout.gif\" width=\"30%\" height=\"30%\"></div>\n",
    "\n",
    "http://ikuz.eu/wp-content/uploads/2015/02/breakout.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Markov Decision Process 2.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 10.07.31 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 10.33.19 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Szepesvári \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Algorithms-for-Reinforcement-Learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 10.16.44 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 10.20.25 AM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 12.57.43 PM.png\" width=\"80%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 12.57.53 PM.png\" width=\"60%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 12.58.01 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 12.58.10 PM.png\" width=\"70%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2018-12-18 at 12.58.41 PM.png\" width=\"40%\"></div>\n",
    "\n",
    "Silver\n",
    "[1](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) \n",
    "[pdf](http://localhost:8888/notebooks/Dropbox/Paper/Reinforcement_Learning_by_David_Silver_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP in Andrew Ng's Lecture 16 (CS229)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2017-11-18 at 8.09.22 PM.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "Ng\n",
    "[16](https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16)\n",
    "[demo](http://heli.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "0&1&2&3\\\\\n",
    "4&\\mbox{W}&5&6\\\\\n",
    "7&8&9&10\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "states = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "n_states = len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0: Left\n",
    "\n",
    "- 1: Right\n",
    "\n",
    "- 2: Up\n",
    "\n",
    "- 3: Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3] # left, right, up, down\n",
    "n_actions = len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transition probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You move according to your action with 80$\\%$ probability. \n",
    "\n",
    "- Your move may have a left and right one click error with 10$\\%$ probability each. \n",
    "\n",
    "- If there is a barrier against your move, your move bounds back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# transition probabilities\n",
    "P = np.empty((n_states, n_actions, n_states))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- -0.02 for each action.\n",
    "\n",
    "- If you reach the state 3, you win and get the final reward 1 in addition.\n",
    "\n",
    "- If you reach the state 6, you lose and get the final reward -1 in addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# rewards\n",
    "R = -0.02 * np.ones((n_states, n_actions, n_states)) \n",
    "R[:,:,3] = - 0.02 + 1\n",
    "R[:,:,6] = - 0.02 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\gamma=0.99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"../img/2007-164-water-policy.jpg\" width=\"40%\" height=\"10%\"></div>\n",
    "\n",
    "- When you are at state $s$, there are many actions you can choose.\n",
    "\n",
    "- Policy descibe how you choose your action.\n",
    "\n",
    "http://www.inkcinct.com.au/web-pages/cartoons/past/2007/2007-164-water-policy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Screenshot+2016-12-16+15.11.27.png\" width=\"60%\" height=\"10%\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "http://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/56381d00e4b05b1abc31cd96/58546ed09de4bb1925de9469/1494102176399/Screenshot+2016-12-16+15.11.27.png?format=1000w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bad policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Downarrow&\\mbox{W}&\\Rightarrow&-1\\\\\n",
    "\\Rightarrow&\\Rightarrow&\\Uparrow&\\Uparrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.empty((n_states, n_actions))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,0,1]\n",
    "policy[5,:] = [0,1,0,0]\n",
    "policy[6,:] = [0,1,0,0]\n",
    "policy[7,:] = [0,1,0,0]\n",
    "policy[8,:] = [0,1,0,0]\n",
    "policy[9,:] = [0,0,1,0]\n",
    "policy[10,:] = [0,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = 0.25*np.ones((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{array}{cccccccc}\n",
    "\\Rightarrow&\\Rightarrow&\\Rightarrow&1\\\\\n",
    "\\Uparrow&\\mbox{W}&\\Uparrow&-1\\\\\n",
    "\\Uparrow&\\Leftarrow&\\Leftarrow&\\Leftarrow\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = np.empty((n_states, n_actions))\n",
    "policy[0,:] = [0,1,0,0]\n",
    "policy[1,:] = [0,1,0,0]\n",
    "policy[2,:] = [0,1,0,0]\n",
    "policy[3,:] = [0,1,0,0]\n",
    "policy[4,:] = [0,0,1,0]\n",
    "policy[5,:] = [0,0,1,0]\n",
    "policy[6,:] = [0,0,1,0]\n",
    "policy[7,:] = [0,0,1,0]\n",
    "policy[8,:] = [1,0,0,0]\n",
    "policy[9,:] = [1,0,0,0]\n",
    "policy[10,:] = [1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# policy\n",
    "if 1: \n",
    "    # bad policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((n_states, n_actions))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((n_states, n_actions))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((n_states,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate random samples from discrete distribution - np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x        = [ -3,  -1,   1,   2,   5]\n",
    "pmf      = [0.1, 0.1, 0.1, 0.5, 0.2]\n",
    "x_sample = np.random.choice(x, p=pmf, size=(1000,)) \n",
    "\n",
    "plt.hist(x_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Screen Shot 2019-01-29 at 10.03.50 AM.png\" width=\"80%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  1, a: 3, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 3, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 2, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 0, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 0, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 1, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 1, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 3, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 3, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 1, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 1, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 1, r: -0.02, s1:  2, done: 0, info: prob 0.25\n",
      "s:  2, a: 1, r: -0.02, s1:  2, done: 0, info: prob 0.25\n",
      "s:  2, a: 0, r: -0.02, s1:  5, done: 0, info: prob 0.25\n",
      "s:  5, a: 1, r: -1.02, s1:  6, done: 0, info: prob 0.25\n",
      "final reward -1 obtained and lose the game!\n",
      "s:  5, a: 0, r: -0.02, s1:  5, done: 0, info: prob 0.25\n",
      "s:  5, a: 3, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 2, r: -0.02, s1:  5, done: 0, info: prob 0.25\n",
      "s:  5, a: 1, r: -0.02, s1:  2, done: 0, info: prob 0.25\n",
      "s:  2, a: 0, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 0, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 0, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 2, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 0, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  1, done: 0, info: prob 0.25\n",
      "s:  1, a: 3, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 2, r: -0.02, s1:  0, done: 0, info: prob 0.25\n",
      "s:  0, a: 3, r: -0.02, s1:  4, done: 0, info: prob 0.25\n",
      "s:  4, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 3, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 0, r: -0.02, s1:  7, done: 0, info: prob 0.25\n",
      "s:  7, a: 1, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 1, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 0, r: -0.02, s1:  8, done: 0, info: prob 0.25\n",
      "s:  8, a: 1, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 0, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 3, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 3, r: -0.02, s1: 10, done: 0, info: prob 0.25\n",
      "s: 10, a: 0, r: -0.02, s1:  9, done: 0, info: prob 0.25\n",
      "s:  9, a: 3, r: -0.02, s1: 10, done: 0, info: prob 0.25\n",
      "s: 10, a: 2, r: -1.02, s1:  6, done: 0, info: prob 0.25\n",
      "final reward -1 obtained and lose the game!\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 2\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_states = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "n_actions = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((n_states, n_actions, n_states))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((n_states, n_actions, n_states)) \n",
    "R[:,:,3] = - 0.02 + 1\n",
    "R[:,:,6] = - 0.02 - 1\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 1: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((n_states, n_actions))\n",
    "elif 1: \n",
    "    # optimal policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((n_states, n_actions))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((n_states,1))\n",
    "\n",
    "# MDP simulation\n",
    "msg = \"s: {:2}, a: {}, r: {:5.2f}, s1: {:2}, done: {:1}, info: {}\" \n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "\n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # probaility of choosing action a under the current policy\n",
    "        prob = policy[s, a]\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "        \n",
    "        # print current situation\n",
    "        msg_print = msg.format(s, a, R[s, a, s1], s1, done, \"prob \"+str(prob))\n",
    "        print(msg_print)\n",
    "        \n",
    "        if (s1 == 3):\n",
    "            # if win, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "            # print final win comment\n",
    "            print('final reward 1 obtained and win the game!')\n",
    "        elif (s1 == 6):\n",
    "            # if lose, \n",
    "            # ready to break while loop by letting done = True\n",
    "            done = True\n",
    "            # print final lose comment\n",
    "            print('final reward -1 obtained and lose the game!')\n",
    "        else:\n",
    "            # if game is not over, \n",
    "            # continue playing game\n",
    "            s = s1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"../img/Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy.png\" width=\"60%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.947\n"
     ]
    }
   ],
   "source": [
    "# Simulation of MDP in Andrew Ng's Lecture 16 - Success rate of optimal policy\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set parameters ###############################################################\n",
    "epoch = 1000\n",
    "# set parameters ###############################################################\n",
    "\n",
    "# state\n",
    "states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_states = len(states)\n",
    "\n",
    "# action\n",
    "actions = [0, 1, 2, 3]  # left, right, up, down\n",
    "n_actions = len(actions)\n",
    "\n",
    "# transition probabilities\n",
    "P = np.empty((n_states, n_actions, n_states))\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 0, 0, :] = [ .9,  0,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 1, :] = [ .1, .8,  0,  0, .1,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 2, :] = [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 0, 3, :] = [ .1, .1,  0,  0, .8,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 1, 0, :] = [ .8, .2,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 1, :] = [  0, .2, .8,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 2, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 1, 3, :] = [ .1, .8, .1,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 2, 0, :] = [  0, .8, .1,  0,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 1, :] = [  0,  0, .1, .8,  0, .1,  0,  0,  0,  0,  0]\n",
    "P[ 2, 2, :] = [  0, .1, .8, .1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 2, 3, :] = [  0, .1,  0, .1,  0, .8,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 3, 0, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 1, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 2, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "P[ 3, 3, :] = [  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 4, 0, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 1, :] = [ .1,  0,  0,  0, .8,  0,  0, .1,  0,  0,  0]\n",
    "P[ 4, 2, :] = [ .8,  0,  0,  0, .2,  0,  0,  0,  0,  0,  0]\n",
    "P[ 4, 3, :] = [  0,  0,  0,  0, .2,  0,  0, .8,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 5, 0, :] = [  0,  0, .1,  0,  0, .8,  0,  0,  0, .1,  0]\n",
    "P[ 5, 1, :] = [  0,  0, .1,  0,  0,  0, .8,  0,  0, .1,  0]\n",
    "P[ 5, 2, :] = [  0,  0, .8,  0,  0, .1, .1,  0,  0,  0,  0]\n",
    "P[ 5, 3, :] = [  0,  0,  0,  0,  0, .1, .1,  0,  0, .8,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 6, 0, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 1, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 2, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "P[ 6, 3, :] = [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 7, 0, :] = [  0,  0,  0,  0, .1,  0,  0, .9,  0,  0,  0]\n",
    "P[ 7, 1, :] = [  0,  0,  0,  0, .1,  0,  0, .1, .8,  0,  0]\n",
    "P[ 7, 2, :] = [  0,  0,  0,  0, .8,  0,  0, .1, .1,  0,  0]\n",
    "P[ 7, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .9, .1,  0,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 8, 0, :] = [  0,  0,  0,  0,  0,  0,  0, .8, .2,  0,  0]\n",
    "P[ 8, 1, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .2, .8,  0]\n",
    "P[ 8, 2, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "P[ 8, 3, :] = [  0,  0,  0,  0,  0,  0,  0, .1, .8, .1,  0]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[ 9, 0, :] = [  0,  0,  0,  0,  0, .1,  0,  0, .8, .1,  0]\n",
    "P[ 9, 1, :] = [  0,  0,  0,  0,  0, .1,  0,  0,  0, .1, .8]\n",
    "P[ 9, 2, :] = [  0,  0,  0,  0,  0, .8,  0,  0, .1,  0, .1]\n",
    "P[ 9, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0, .1, .8, .1]\n",
    "\n",
    "#                0   1   2   3   4   5   6   7   8   9  10\n",
    "P[10, 0, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0, .8, .1]\n",
    "P[10, 1, :] = [  0,  0,  0,  0,  0,  0, .1,  0,  0,  0, .9]\n",
    "P[10, 2, :] = [  0,  0,  0,  0,  0,  0, .8,  0,  0, .1, .1]\n",
    "P[10, 3, :] = [  0,  0,  0,  0,  0,  0,  0,  0,  0, .1, .9]\n",
    "\n",
    "# rewards\n",
    "R = -0.02 * np.ones((n_states, n_actions, n_states)) \n",
    "R[:,:,3] = - 0.02 + 1\n",
    "R[:,:,6] = - 0.02 - 1\n",
    "    \n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy\n",
    "if 0: \n",
    "    # bad policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,0,1]\n",
    "    policy[5,:] = [0,1,0,0]\n",
    "    policy[6,:] = [0,1,0,0]\n",
    "    policy[7,:] = [0,1,0,0]\n",
    "    policy[8,:] = [0,1,0,0]\n",
    "    policy[9,:] = [0,0,1,0]\n",
    "    policy[10,:] = [0,0,1,0]\n",
    "elif 0: \n",
    "    # random policy\n",
    "    policy = 0.25*np.ones((n_states, n_actions))\n",
    "elif 0: \n",
    "    # optimal policy \n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "elif 1: \n",
    "    # optimal policy + noise \n",
    "    # we use optimal policy with probability 1/(1+ep)\n",
    "    # we use random policy with probability ep/(1+ep)\n",
    "    ep = 0.1\n",
    "    policy = np.empty((n_states, n_actions))\n",
    "    policy[0,:] = [0,1,0,0]\n",
    "    policy[1,:] = [0,1,0,0]\n",
    "    policy[2,:] = [0,1,0,0]\n",
    "    policy[3,:] = [0,1,0,0]\n",
    "    policy[4,:] = [0,0,1,0]\n",
    "    policy[5,:] = [0,0,1,0]\n",
    "    policy[6,:] = [0,0,1,0]\n",
    "    policy[7,:] = [0,0,1,0]\n",
    "    policy[8,:] = [1,0,0,0]\n",
    "    policy[9,:] = [1,0,0,0]\n",
    "    policy[10,:] = [1,0,0,0]\n",
    "    policy = policy + (ep/4)*np.ones((n_states, n_actions))\n",
    "    policy = policy / np.sum(policy, axis=1).reshape((n_states,1))\n",
    "\n",
    "# MDP simulation\n",
    "simulation_history = []\n",
    "for _ in range(epoch):\n",
    "\n",
    "    # indicate game is not over yet\n",
    "    done = False\n",
    "    \n",
    "    # choose initial state randomly, not from 3 or 6\n",
    "    s = np.random.choice([0, 1, 2, 4, 5, 7, 8, 9, 10])  \n",
    "    \n",
    "    while not done:\n",
    "        # choose action using current policy\n",
    "        a = np.random.choice(actions, p=policy[s, :])\n",
    "        \n",
    "        # choose next state using transition probabilities\n",
    "        s1 = np.random.choice(states, p=P[s, a, :])\n",
    "\n",
    "        if s1 == 3:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(1.)\n",
    "        elif s1 == 6:\n",
    "            # if game is over, \n",
    "            # ready to break while loop by letting done = True\n",
    "            # append end result to simulation_history \n",
    "            done = True\n",
    "            simulation_history.append(0.)\n",
    "        else:\n",
    "            # if game is not over, continue playing game\n",
    "            s = s1\n",
    "\n",
    "history = np.cumsum(simulation_history) / (np.arange(epoch) + 1)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "print(\"Success rate: {}\".format(history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
